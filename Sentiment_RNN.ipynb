{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "\n",
    "# Read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
      "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n",
      "\n",
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "# Print first 2000 characters in reviews\n",
    "print(reviews[:2000])\n",
    "print()\n",
    "\n",
    "# Print first 20 characters in labels\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from string import punctuation\n",
    "\n",
    "# Print list of punctuation characters\n",
    "print(punctuation)\n",
    "\n",
    "# Clean up the reviews: lowercase, remove punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation]) # get rid of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split by new lines and spaces\n",
    "reviews_split = all_text.split('\\n') # split by new lines and spaces\n",
    "all_text = ' '.join(reviews_split) # join all the reviews together into one long string\n",
    "\n",
    "# Create a list of words\n",
    "words = all_text.split() # split long string by spaces into words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate pre-processing on first 30 words\n",
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  74072\n",
      "\n",
      "Tokenized review: \n",
      " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers (do not use 0 as it is used for padding later on)\n",
    "counts = Counter(words) # create a dictionary of word counts; maps most common words to smallest integers\n",
    "vocab = sorted(counts, key=counts.get, reverse=True) # sort words by frequency/commonality\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} # for each word in the vocab list, it will return a tuple containing the index and the word\n",
    "\n",
    "# Alternative way to create vocab_to_int\n",
    "# words_unique = np.unique(words)\n",
    "# vocab_to_int = dict(zip(words_unique, range(1, len(words_unique)+1)))\n",
    "\n",
    "# Use the dict to tokenize each review in reviews_split\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    word_token = [vocab_to_int[word] for word in review.split()] # tokenize each word\n",
    "    reviews_ints.append(word_token) # append tokenized word to reviews_ints\n",
    "\n",
    "# Print stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# Print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n",
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n",
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n",
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n",
      " Label: positive, Encoded label: 1\n",
      " Label: negative, Encoded label: 0\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "labels_split = labels.split('\\n') # split at new lines\n",
    "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split]) # convert 'positive' to 1 and 'negative' to 0\n",
    "\n",
    "# Check if labels are correct\n",
    "for i in range(10):\n",
    "    print(f' Label: {labels_split[i]}, Encoded label: {encoded_labels[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "# Outlier review stats\n",
    "review_lens = Counter([len(x) for x in reviews_ints]) # Dict of review lengths and their frequency\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  25001\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "# Print num reviews before removing outliers\n",
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "# Remove any reviews/labels with zero length from the reviews_ints list.\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0] # get indices of any reviews with length 0\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx] # remove 0-length reviews \n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx]) # remove 0-length labels\n",
    "\n",
    "# Print num reviews after removing outliers\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    # Getting the correct rows x cols shape\n",
    "    # this creats an array of zeros with the shape of the number of reviews by the sequence length\n",
    "    features = np.zeros((len(reviews_ints), # rows = number of reviews\n",
    "                        seq_length), # cols = desired sequence length\n",
    "                        dtype=int)\n",
    "\n",
    "    # For each review, I grab that review and insert it into the array of zeros\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        # if the review is longer than the sequence length, it will be truncated ([:seq_length])\n",
    "        # if the review is shorter than the sequence length, the previously filled in zeros will remain = it will be padded with zeros\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length] # start at the end of the array [i] and work backwards towards the beginning of the array [-len(row):]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   330   578    34     3   162   748  2731     9   325]\n",
      " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
      " [    1    20     6    76    40     6    58    81    95     5]\n",
      " [   54    10    84   329 26230 46427    63    10    14   614]\n",
      " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   40    26   109 17952  1422     9     1   327     4   125]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   10   499     1   307 10399    55    74     8    13    30]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Test implementation\n",
    "seq_length = 200\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "# Test statements\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# Print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "# Define fraction to keep for training\n",
    "split_frac = 0.8\n",
    "\n",
    "# Split data into training, validation, and test data (features and labels, x and y)\n",
    "split_idx = int(len(features)*split_frac) # get index at which to split: number of reviews times the split fraction (80%)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:] # use split index to split features into training and remaining\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5) # get half the indeces to split the remaining data into validation and test\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:] # split all left until the test index into validation and all right into test set\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## Print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# Set batch size for data loaders\n",
    "batch_size = 50\n",
    "\n",
    "# Shuffle data, so that model doesn't learn anything about ordering of the data, and instead focuses on content\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[   1, 2797, 2912,  ..., 9251,    4, 1374],\n",
      "        [   0,    0,    0,  ...,   35, 1069,  226],\n",
      "        [   0,    0,    0,  ...,   43,   44,    8],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1511,   71,  350],\n",
      "        [   0,    0,    0,  ...,   16,  339,  572],\n",
      "        [   0,    0,    0,  ...,   39,   10,  120]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "# Print shape of batch of features and labels\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch.nn as nn\n",
    "\n",
    "# Defining the model\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        # Defining parameters for the model\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding words for dimensionality reduction (altnernative to one-hot encoding, which is inefficient for large vocabularies)\n",
    "        # creates a lookup table that maps words to vectors of a specified size\n",
    "        self.embedding = nn.Embedding(vocab_size, # number of rows: word integers\n",
    "                                      embedding_dim) # number of columns: the embedding dimension\n",
    "        \n",
    "        # Defining the LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, # input size is equal to the output of the embedding layer (i.e. embedding dimension)\n",
    "                            hidden_dim, # output size is equal to the hidden dimension/number of units in the hidden layer\n",
    "                            n_layers,\n",
    "                            dropout=drop_prob, \n",
    "                            batch_first=True) # True because we are using DataLoaders to batch our data like that\n",
    "        \n",
    "        # Define dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Define linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        # Extracting the size of the first dimension of the tensor x and assigning it to the variable batch_size\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Embed text and pass through LSTM layer\n",
    "        x = x.long() # converts the elements of the tensor x to a 64-bit integer data type (torch.int64 in the case of PyTorch).\n",
    "        embeds = self.embedding(x) # run the input x through the embedding layer\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden) # run the embedding output through the LSTM layer\n",
    "        \n",
    "        # Stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Pass through dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out) # run the LSTM output through the dropout layer\n",
    "        out = self.fc(out) # run the dropout output through the fully-connected layer\n",
    "        \n",
    "        # Activate: Sigmoid function\n",
    "        sig_out = self.sig(out) # run the fully-connected output through the sigmoid function\n",
    "        \n",
    "        # Reshape to be batch_size first (to feed into next LSTM layer)\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # Return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1 # one output neuron/class score for positive/negative\n",
    "embedding_dim = 400 # just a smaller representation of vocabulary of 70k words --> any value between like 200 and 500 should work\n",
    "hidden_dim = 256 # 256 hidden features should be enough to distinguish between positive and negative reviews.\n",
    "n_layers = 2 # 2 layers should be enough to distinguish between positive and negative reviews.\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define learning rate\n",
    "lr=0.001\n",
    "\n",
    "# Set loss and optimization functions\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss, applies cross entropy loss to a single value between 0 and 1.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Train Loss: 0.647868... Val Loss: 0.641024\n",
      "Epoch: 1/4... Step: 200... Train Loss: 0.576166... Val Loss: 0.598000\n",
      "Epoch: 1/4... Step: 300... Train Loss: 0.555865... Val Loss: 0.621067\n",
      "Epoch: 1/4... Step: 400... Train Loss: 0.604562... Val Loss: 0.549225\n",
      "Epoch: 2/4... Step: 500... Train Loss: 0.284617... Val Loss: 0.516605\n",
      "Epoch: 2/4... Step: 600... Train Loss: 0.506833... Val Loss: 0.491125\n",
      "Epoch: 2/4... Step: 700... Train Loss: 0.505795... Val Loss: 0.485445\n",
      "Epoch: 2/4... Step: 800... Train Loss: 0.321016... Val Loss: 0.449118\n",
      "Epoch: 3/4... Step: 900... Train Loss: 0.244150... Val Loss: 0.593418\n",
      "Epoch: 3/4... Step: 1000... Train Loss: 0.350111... Val Loss: 0.509438\n",
      "Epoch: 3/4... Step: 1100... Train Loss: 0.247316... Val Loss: 0.442915\n",
      "Epoch: 3/4... Step: 1200... Train Loss: 0.268553... Val Loss: 0.433219\n",
      "Epoch: 4/4... Step: 1300... Train Loss: 0.303633... Val Loss: 0.515278\n",
      "Epoch: 4/4... Step: 1400... Train Loss: 0.311155... Val Loss: 0.526051\n",
      "Epoch: 4/4... Step: 1500... Train Loss: 0.194953... Val Loss: 0.505703\n",
      "Epoch: 4/4... Step: 1600... Train Loss: 0.287554... Val Loss: 0.567540\n"
     ]
    }
   ],
   "source": [
    "# Set training params\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# Move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "# Set model to train mode\n",
    "net.train()\n",
    "\n",
    "# Train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # Initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # Batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise this would backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # Clear the gradients of all optimized variables\n",
    "        net.zero_grad()\n",
    "\n",
    "        # Get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # Calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float()) # making sure that our outputs are squeezed so that they do not have an empty dimension\n",
    "        loss.backward() # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip) # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        optimizer.step() # Perform a single optimization step (parameter update)\n",
    "\n",
    "        # Get loss stats every few batches\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "\n",
    "            # Set model to evaluation mode\n",
    "            net.eval()\n",
    "\n",
    "            for inputs, labels in valid_loader:\n",
    "                # Creating new variables for the hidden state, otherwise this would backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                # Get the output from the model\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())  # making sure that our outputs are squeezed so that they do not have an empty dimension\n",
    "                val_losses.append(val_loss.item()) # append the validation loss to the list of validation losses\n",
    "\n",
    "            # Set model back to train mode\n",
    "            net.train()\n",
    "\n",
    "            # Print training and validation loss stats\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Train Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.558\n",
      "Test accuracy: 0.796\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# Init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# Iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise this would backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # Get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # Calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # Convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # Compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "## Print stats\n",
    "# Avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# Accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 247, 18, 10, 28, 108, 113, 14, 388, 2, 10, 181, 60, 273, 144, 11, 18, 68, 76, 113, 2, 1, 410, 14, 539]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# Define function to tokenize review\n",
    "def tokenize_review(test_review):\n",
    "    # Coverting test review to lowercase\n",
    "    test_review = test_review.lower()\n",
    "\n",
    "    # Getting rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # Splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # Tokenizing words using the vocab_to_int dictionary\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int.get(word, 0) for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# Tokenize test review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   1 247  18  10  28\n",
      "  108 113  14 388   2  10 181  60 273 144  11  18  68  76 113   2   1 410\n",
      "   14 539]]\n"
     ]
    }
   ],
   "source": [
    "# Test padding of test review\n",
    "seq_length=200 # use the same sequence length that was trained on\n",
    "features = pad_features(test_ints, seq_length)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# Test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print custom response based on whether test_review is pos/neg\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "    ''' Prints out whether a give review is predicted to be \n",
    "        positive or negative in sentiment, using a trained model.\n",
    "        \n",
    "        params:\n",
    "        net - A trained net \n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # Pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # Convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    # Get batch size from feature tensor\n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # Get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # Convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "\n",
    "    # Printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # Print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing predict function on both negative and positive test reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.004021\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "# Define negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
    "\n",
    "# Call function on negative test review\n",
    "seq_length=200\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.993114\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "# Define positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
    "\n",
    "# Call function on negative test review\n",
    "seq_length=200\n",
    "predict(net, test_review_pos, seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
